{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='<directory to store generated job scripts>'\n",
    "# os.mkdir(dir)\n",
    "# %rm <directory to store generated job scripts>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs=['fp32-batch2','fp32-batch1','fp16-batch2']\n",
    "scriptfile='<path to climate-seg-benchmark/deeplab-tf/deeplab-tf-train.py>'\n",
    "\n",
    "with open(scriptfile,'r') as f:\n",
    "    lns=f.readlines()\n",
    "#     print(len(lns))       \n",
    "    ln_tmpl=[i+1 for i,x in enumerate(lns) if '_ = sess.run([tmpl],feed_dict={handle: trn_handle})' in x ][0]\n",
    "    ln_trainop=[i+1 for i,x in enumerate(lns) if '_ = sess.run([train_op],feed_dict={handle: trn_handle})' in x ][1]\n",
    "    \n",
    "for subdir in subdirs:\n",
    "    subdirfull=os.path.join(dir,subdir)\n",
    "    if not os.path.isdir(subdirfull):\n",
    "        os.mkdir(subdirfull)\n",
    "    \n",
    "    if subdir == 'fp32-batch2':\n",
    "        filenames=[subdir+'-'+x+'-'+y+'.sh' for x in ['fw','fwbw'] for y in ['amp']]\n",
    "    else:\n",
    "        filenames=[subdir+'-'+x+'-'+y+'.sh' for x in ['fw','fwbw'] for y in ['amp','noamp']]\n",
    "    \n",
    "    for fn in filenames:\n",
    "        with open(os.path.join(subdirfull,fn),'w') as f:\n",
    "            f.write('''#!/bin/bash\n",
    "#SBATCH -J {fn}\\n'''.format(fn=fn.split('.')[0]))\n",
    "            f.write('''#SBATCH -C gpu\n",
    "#SBATCH --gres=gpu:1\n",
    "##SBATCH --exclusive\n",
    "#SBATCH -t 04:00:00\n",
    "\n",
    "# Job parameters\n",
    "do_stage=false\n",
    "nvalid=0\n",
    "ntest=0\n",
    "epochs=1\\n''')\n",
    "            f.write('''ntrain={ntrain}\n",
    "batch={batch}\n",
    "prec={prec}\\n'''.format(ntrain=6*int(subdir.split('-')[1].split('batch')[1]),\\\n",
    "                        batch=subdir.split('-')[1].split('batch')[1],\\\n",
    "                        prec=subdir.split('-')[0].split('fp')[1]))\n",
    "            f.write('''grad_lag=1\n",
    "scale_factor=0.1\n",
    "loss_type=weighted  \n",
    "\n",
    "# Parse command line options\n",
    "while (( \"$#\" )); do\n",
    "    case \"$1\" in\n",
    "        --ntrain)\n",
    "            ntrain=$2\n",
    "            shift 2\n",
    "            ;;\n",
    "        --nvalid)\n",
    "            nvalid=$2\n",
    "            shift 2\n",
    "            ;;\n",
    "        --ntest)\n",
    "            ntest=$2\n",
    "            shift 2\n",
    "            ;;\n",
    "        --epochs)\n",
    "            epochs=$2\n",
    "            shift 2\n",
    "            ;;\n",
    "        --dummy)\n",
    "            other_train_opts=\"--dummy_data\"\n",
    "            shift\n",
    "            ;;\n",
    "        -*|--*=)\n",
    "            echo \"Error: Unsupported flag $1\" >&2\n",
    "            exit 1\n",
    "            ;;\n",
    "    esac\n",
    "done\n",
    "\n",
    "#load modules\n",
    "module load tensorflow/gpu-1.15.0-py37\n",
    "\n",
    "export OMP_PLACES=threads\n",
    "export OMP_PROC_BIND=spread\n",
    "export HDF5_USE_FILE_LOCKING=FALSE\n",
    "\n",
    "# Setup directories\n",
    "datadir=<path to data/climseg-data-duplicated>\\n''')\n",
    "            f.write('''out_dir=$SCRATCH/climate-seg-benchmark/TF-{fn}-$SLURM_JOBID/\\n'''.format(fn=fn.split('.')[0]))\n",
    "            f.write('''run_dir=${out_dir}\n",
    "mkdir -p ${out_dir}\n",
    "\n",
    "# Prepare the run directory\n",
    "script_dir=<path to climate-seg-benchmark/run_scripts>\n",
    "cp ${script_dir}/stage_in_parallel.sh ${run_dir}/\n",
    "cp ${script_dir}/../utils/parallel_stagein.py ${run_dir}/\n",
    "cp ${script_dir}/../utils/graph_flops.py ${run_dir}/\n",
    "cp ${script_dir}/../utils/tracehook.py ${run_dir}/\n",
    "cp ${script_dir}/../utils/common_helpers.py ${run_dir}/\n",
    "cp ${script_dir}/../utils/data_helpers.py ${run_dir}/\n",
    "cp ${script_dir}/../deeplab-tf/deeplab-tf-train.py ${run_dir}/\n",
    "cp ${script_dir}/../deeplab-tf/deeplab-tf-inference.py ${run_dir}/\n",
    "cp ${script_dir}/../deeplab-tf/deeplab_model.py ${run_dir}/\\n''')\n",
    "            f.write('''cp $0 ${{run_dir}}/{fn}\n",
    "cd ${{run_dir}}\\n'''.format(fn=fn))\n",
    "            if 'noamp' in fn:\n",
    "                f.write('''sed -e '/enable_mixed_precision_graph_rewrite/ s/^#*/#/' -i common_helpers.py\\n''')\n",
    "            else:\n",
    "                f.write('''#sed -e '/enable_mixed_precision_graph_rewrite/ s/^#*/#/' -i common_helpers.py\\n''')\n",
    "            if 'fwbw' in fn:\n",
    "                f.write('''#sed -i '{ln_tmpl}{{s/^#//}}' deeplab-tf-train.py\n",
    "#sed -i '{ln_trainop}{{s/^/#/}}' deeplab-tf-train.py\\n'''.format(ln_tmpl=ln_tmpl,ln_trainop=ln_trainop))\n",
    "            else:\n",
    "                f.write('''sed -i '{ln_tmpl}{{s/^#//}}' deeplab-tf-train.py\n",
    "sed -i '{ln_trainop}{{s/^/#/}}' deeplab-tf-train.py\\n'''.format(ln_tmpl=ln_tmpl,ln_trainop=ln_trainop))\n",
    "\n",
    "            f.write('''metrics=\"sm__cycles_elapsed.avg.per_second \\\\\n",
    "sm__cycles_elapsed.avg \\\\\n",
    "sm__inst_executed_pipe_tensor.sum \\\\\n",
    "sm__sass_thread_inst_executed_op_fadd_pred_on.sum \\\\\n",
    "sm__sass_thread_inst_executed_op_ffma_pred_on.sum \\\\\n",
    "sm__sass_thread_inst_executed_op_fmul_pred_on.sum \\\\\n",
    "sm__sass_thread_inst_executed_op_hadd_pred_on.sum \\\\\n",
    "sm__sass_thread_inst_executed_op_hfma_pred_on.sum \\\\\n",
    "sm__sass_thread_inst_executed_op_hmul_pred_on.sum \\\\\n",
    "dram__bytes.sum \\\\\n",
    "lts__t_bytes.sum \\\\\n",
    "l1tex__t_bytes.sum \"\n",
    "\n",
    "#export TF_CUDNN_USE_AUTOTUNE=0\n",
    "#export TF_CUDNN_DETERMINISTIC=1\n",
    "export HOROVOD_FUSION_THRESHOLD=0\n",
    "\n",
    "for metric in ${metrics}; do\n",
    "\n",
    "profilestring=\"/usr/common/software/cuda/11.0.167/bin/nv-nsight-cu-cli --profile-from-start off --metrics ${metric} --csv --kernel-base demangled\"\n",
    "\n",
    "# Run the training\n",
    "if [ $ntrain -ne 0 ]; then\n",
    "    echo \"Starting Training\"\n",
    "    srun -n1 -u ${profilestring} `which python` -u deeplab-tf-train.py \\\\\n",
    "        --datadir_train ${datadir}/train \\\\\n",
    "        --train_size ${ntrain} \\\\\n",
    "        --validation_size ${nvalid} \\\\\n",
    "        --datadir_validation ${datadir}/validation \\\\\n",
    "        --disable_checkpoint \\\\\n",
    "        --epochs $epochs \\\\\n",
    "        --fs \"global\" \\\\\n",
    "        --loss $loss_type \\\\\n",
    "        --optimizer opt_type=LARC-Adam,learning_rate=0.0001,gradient_lag=${grad_lag} \\\\\n",
    "        --model \"resnet_v2_50\" \\\\\n",
    "        --scale_factor $scale_factor \\\\\n",
    "        --batch $batch \\\\\n",
    "        --decoder \"deconv1x\" \\\\\n",
    "        --device \"/device:cpu:0\" \\\\\n",
    "        --dtype \"float${prec}\" \\\\\n",
    "        --label_id 0 \\\\\n",
    "        --data_format \"channels_first\" \\\\\n",
    "        --use_batchnorm \\\\\n",
    "        --disable_imsave \\\\\n",
    "        $other_train_opts 2>&1 > out.fp${prec}.lag${grad_lag}.train.${metric}\n",
    "fi\n",
    "\n",
    "done \\n''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
